{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4b07a8",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance for Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f06a5",
   "metadata": {},
   "source": [
    "### Excercise: Comparing Model Performance\n",
    "Load a dataset of choice, ameanible to linear regression analysis. You can use the data from the last workshop. Split the data into train - validation - and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f84aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/Sari/Documents/Code/Semester 4 workbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13da3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('FiveThirtyEight_president_polls_02.02.2024.csv')\n",
    "\n",
    "# make the states into dummies - we will work with them as dummies from here on \n",
    "state_dummies = pd.get_dummies(df['state'], prefix='state', drop_first=True)\n",
    "state_dummies = state_dummies.astype(float) \n",
    "\n",
    "# concatenate the dummy variables with the DataFrame\n",
    "df = pd.concat([df, state_dummies], axis=1)\n",
    "df = df.drop('state', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8abc44",
   "metadata": {},
   "source": [
    "### 1a: Split your dataset into three parts, namely training, validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9c53645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sari/Documents/Code/Semester 4 workbooks/venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_portion = 0.8\n",
    "validation_portion = 0.9\n",
    "\n",
    "\n",
    "data_portions = [\n",
    "    int(train_portion * len(df)),\n",
    "    int(validation_portion * len(df)),\n",
    "]\n",
    "\n",
    "train_set, validate_set, test_set = np.split(df.sample(frac=1), data_portions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eba7008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Create the design matrices - add a constant to the training and validation (and test, whilst we are at it) design matrices\n",
    "train_X = train_set.drop('poll_id', axis=1)\n",
    "train_y = train_set['poll_id']\n",
    "\n",
    "train_X_full = sm.add_constant( train_X )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af180f",
   "metadata": {},
   "source": [
    "### 1b:  Compare 3 models on their performance on the validation set\n",
    "\n",
    "Define 3 models, one `unsaturated` which uses only the intercept; one `fully saturated` which represents the most complex model you can generate, and one in between. Fit each model to the training data, and generate predictions for each model on the test_set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def fit_predict_evaluate(train_X, train_y, valid_X, valid_y, credibility_pct=95, n_sims = 10000):\n",
    "    \n",
    "    # Fit the model\n",
    "    model = sm.OLS(train_y, train_X).fit()\n",
    "    \n",
    "    # Step 2: Predict new values\n",
    "    new_predictions = ...\n",
    "    \n",
    "    # Step 3: Calculate standard errors of the predictions on the validation set\n",
    "    cov_matrix = ...\n",
    "    design_matrix = ...\n",
    "    variances = np.diag(np.dot(np.dot(design_matrix, cov_matrix), design_matrix.T))\n",
    "    std_errors_predictions = ...\n",
    "    \n",
    "    # Generate posterior samples\n",
    "    new_predictions_array = np.array(new_predictions)[:, None]  \n",
    "    std_errors_predictions_array = np.array(std_errors_predictions)[:, None]  \n",
    "    \n",
    "    predicted_samples = np.random.normal(loc=new_predictions_array,\n",
    "                                         scale=std_errors_predictions_array,\n",
    "                                         size=(..., n_sims))\n",
    "    \n",
    "    # Calculate estimates summaries\n",
    "    monte_carlo_medians = ...\n",
    "    prediction_intervals = ...\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bias = ...\n",
    "    rmse = ...\n",
    "    correlation = ...\n",
    "    coverage = ...\n",
    "    \n",
    "    # Return results as a dictionary\n",
    "    results = {\n",
    "        'bias': bias,\n",
    "        'rmse': rmse,\n",
    "        'correlation': correlation,\n",
    "        'coverage': coverage,\n",
    "        'predictions': new_predictions,\n",
    "        'prediction_intervals': prediction_intervals\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b020c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate scores for each model - this is an example, you don't have to use the exact same variables\n",
    "score_intercept_only = fit_predict_evaluate(train_X_full[['const']], train_y, valid_X_full[['const']], valid_y, credibility_pct=95) \n",
    "score_national = fit_predict_evaluate(train_X_full[['const','term2', 'real_gdp_pct_growth', 'net_approval']], train_y, valid_X_full[['const','term2', 'real_gdp_pct_growth', 'net_approval']], valid_y, credibility_pct=95) \n",
    "score_saturated = fit_predict_evaluate(train_X_full, train_y, valid_X_full, valid_y, credibility_pct=95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af001e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a table with the scores \n",
    "scores = [\n",
    "    {'name': 'Intercept Only', **score_intercept_only},\n",
    "    {'name': 'National', **score_national},\n",
    "    {'name': 'Saturated', **score_saturated},\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "scores_df_automated = pd.DataFrame(scores)\n",
    "\n",
    "# Rename the 'name' column to 'Score Type' for clarity\n",
    "scores_df_automated.rename(columns={'name': 'Score Type'}, inplace=True)\n",
    "\n",
    "scores_df_automated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2052a1",
   "metadata": {},
   "source": [
    "### 1c:  Comment on the scores\n",
    "\n",
    "Can you identify a model to move forward with ? on what grounds ? \n",
    "What do the differences in performance over the different metrics tell you about each model's strengths and weaknesses ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3af8fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1cd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8d84a",
   "metadata": {},
   "source": [
    "### 1d: For your best performning model, estimate the generalisation error \n",
    "\n",
    "Generate point-estimates and a measure of uncertainty for each error metric. Hint: the most comprehensive way to do this is to generate error distributions. You can do this by calculating each error metric on a separate set of simulated predictions from the empirical predictive posterior distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce420123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model (best performing was National on the RMSE metric)\n",
    "model = sm.OLS(train_y, train_X_full[['const','term2', 'real_gdp_pct_growth', 'net_approval']]).fit()\n",
    "    \n",
    "# Step 2: Predict new values\n",
    "new_predictions = ...\n",
    "    \n",
    "# Step 3: Calculate standard errors of the predictions\n",
    "cov_matrix = ...\n",
    "design_matrix = ...\n",
    "variances = ...\n",
    "std_errors_predictions = ...\n",
    "    \n",
    "# Generate posterior samples\n",
    "new_predictions_array = ...\n",
    "std_errors_predictions_array = ...\n",
    "    \n",
    "predicted_samples = np.random.normal(loc=...,\n",
    "                                     scale=...,\n",
    "                                     size=(..., ...)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "bias_array = ...\n",
    "rmse_array = ...\n",
    "correlation_array = ...\n",
    "# Note: coverage doesn't have a distribution, because it is already a measure which is averaged over simulations. So it is omitted here - we know it from the table above. \n",
    "# Combine metrics into a single array where each metric is a column\n",
    "metrics = {\n",
    "    'Bias': bias_array,\n",
    "    'RMSE': rmse_array,\n",
    "    'Correlation': correlation_array\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2aa9a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the distributions of each error metric\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define metric names\n",
    "metric_names = ['Bias', 'RMSE', 'Correlation']\n",
    "\n",
    "# Define number of metrics to plot\n",
    "n_metrics = len(metric_names)\n",
    "\n",
    "# Adjust rows and columns calculation to avoid empty subplots\n",
    "n_plots = n_metrics  # Total number of plots needed\n",
    "n_rows = 1\n",
    "n_cols = (n_plots + n_rows - 1) // n_rows  # Calculate columns needed, ensuring we have enough space\n",
    "\n",
    "# Adjust the figsize if necessary\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5), squeeze=False)  # Make sure axes is always a 2D array\n",
    "\n",
    "# Flatten axes array for easier iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, name in enumerate(metric_names):\n",
    "    data = metrics[name]  # Access data directly from dictionary\n",
    "    median_val = np.median(data)\n",
    "    confidence_interval = np.percentile(data, [2.5, 97.5])\n",
    "    \n",
    "    ax = axes_flat[i]\n",
    "    ax.hist(data, bins=50, color='skyblue', edgecolor='gray')\n",
    "    ax.axvline(x=median_val, color='green', linestyle='-', label=f'Median: {median_val:.2f}')\n",
    "    ax.axvline(x=confidence_interval[0], color='orange', linestyle='--', label=f'95% CI: [{confidence_interval[0]:.2f}, {confidence_interval[1]:.2f}]')\n",
    "    ax.axvline(x=confidence_interval[1], color='orange', linestyle='--')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', label='Zero line')  # Highlight 0 with a line\n",
    "    ax.set_title(f'Histogram of {name}')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    # Ensure plot encompasses 0 by adjusting xlim if necessary\n",
    "    xlims = ax.get_xlim()\n",
    "    ax.set_xlim(min(xlims[0], 0), max(xlims[1], 0))\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, n_rows * n_cols):\n",
    "    fig.delaxes(axes_flat[j])\n",
    "\n",
    "# Adjust layout for better spacing and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce773b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ef066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
