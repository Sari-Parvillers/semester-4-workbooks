{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66c70ab",
   "metadata": {},
   "source": [
    "# Time Series Part I: Estimation, Inference & Forecasting with ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca97d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some useful libraries\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb90ec4",
   "metadata": {},
   "source": [
    "#### Part 1: Cleaning and Exploring the Covid-19 Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the COVID-19 dataset\n",
    "df = pd.read_csv('forecasts_and_truth.csv')\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6119cc3",
   "metadata": {},
   "source": [
    "#### a) Load the data, filtering the dataset to focus exclusively on incident cases in the whole of the US. Ensure there are no duplicate rows in the resulting dataset. Plot the time series of cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd955a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['target_variable'] == ...]\n",
    "df = df[df['abbreviation'] == ...]\n",
    "df = df[['target_end_date','truth_value']]...\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'target_end_date' column to datetime format\n",
    "df['target_end_date'] = pd.to_datetime(df['target_end_date'])\n",
    "\n",
    "# Plot the time series data\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8c95d",
   "metadata": {},
   "source": [
    "#### b) Does this data look stationary ? How could we transform the series to \"encourage\" compatibility with the stable-variance assumption necessary for time-series analysis ? Apply the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f14858",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462e8c5",
   "metadata": {},
   "source": [
    "#### c) Augment the series with the first 30 lags of the dependent varible. For each lag, calculate the pearson correlation with the most recent value. Plot this correlation again the order of the lag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Augment the series with the first 30 lags\n",
    "lags = 30\n",
    "for lag in range(0, lags + 1):\n",
    "    df[f'lag_{lag}'] = ... .shift(lag)\n",
    "\n",
    "# Calculate the Pearson correlation for each lag\n",
    "correlations = []\n",
    "for lag in range(0, lags + 1):\n",
    "    corr = pearsonr(..., df.dropna()[f'lag_{lag}'])[0]\n",
    "    correlations.append(corr)\n",
    "\n",
    "# Plotting the correlation against the order of the lag\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a89fa",
   "metadata": {},
   "source": [
    "#### d) Interpret the plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646aea0d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d213e29",
   "metadata": {},
   "source": [
    "#### The above plot is called a `correlogram`. It is used to plot the autocorrelation function, which you have just done. The `statmodels` library has its own in-built functions for plotting this, along with another useful plot called the `partial autocorrelation function`. We will get to these in the 2nd time series workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Plotting the Autocorrelation Function (ACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(df['log_truth_value'].dropna(), lags=30, alpha=0.05, title='Autocorrelation Function (ACF) for Log-transformed Series')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plotting the Partial Autocorrelation Function (PACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(df['log_truth_value'].dropna(), lags=30, alpha=0.05, title='Partial Autocorrelation Function (PACF) for Log-transformed Series')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50dc0b",
   "metadata": {},
   "source": [
    "#### Part II: Random Walks, Forecasts and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73d581",
   "metadata": {},
   "source": [
    "#### a) Using a simple random walk, generate a forecast for the 365 days following the last available record in the dataset. Does this generate plausible predictions for the level of new covid cases ?\n",
    "\n",
    "Note: due to the complicated nature of many-steps-ahead time-series forecasting, which can require stepwise prediction and re-calculation of fitted values, errors etc., we trust the ARIMA library from statmodels to generate multi-step prediction intervals. Note that this does not give us access to simulations for each forecast, and hence we have less flexibility. We will see how we can recover some of this flexibility in the following questions. Note that you `can` still do it the old fashioned way, but this is more straightforward and less messy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple ARIMA model (0,1,0) equivalent to a random walk\n",
    "model = ARIMA(df['log_truth_value'], order=(0, 1, 0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast the next 365 days\n",
    "forecast = model_fit.get_forecast(steps=365)\n",
    "\n",
    "# Extract the forecast mean and confidence intervals\n",
    "forecast_mean = forecast.predicted_mean\n",
    "conf_int = forecast.conf_int(alpha=0.05)  # 95% confidence interval\n",
    "\n",
    "# Calculate the 70% and 90% confidence intervals\n",
    "conf_int_70 = forecast.conf_int(alpha=0.30)  # 70% confidence interval\n",
    "conf_int_90 = forecast.conf_int(alpha=0.10)  # 90% confidence interval\n",
    "\n",
    "# Convert forecast index to match the dates if necessary\n",
    "forecast_dates = pd.date_range(start=df['target_end_date'].iloc[-1] + pd.Timedelta(days=1), periods=365)\n",
    "\n",
    "# Plot the historical data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['target_end_date'], df['log_truth_value'], marker='o', linestyle='-', color='black', markersize=4, label='Historical Data')\n",
    "\n",
    "# Plot the forecast\n",
    "plt.plot(forecast_dates, forecast_mean, color='blue', label='Forecast')\n",
    "\n",
    "# Plot the confidence intervals\n",
    "plt.fill_between(forecast_dates, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='grey', alpha=1, label='95% Confidence Interval')\n",
    "plt.fill_between(forecast_dates, conf_int_90.iloc[:, 0], conf_int_90.iloc[:, 1], color='lightcoral', alpha=1, label='90% Confidence Interval')\n",
    "plt.fill_between(forecast_dates, conf_int_70.iloc[:, 0], conf_int_70.iloc[:, 1], color='red', alpha=1, label='70% Confidence Interval')\n",
    "\n",
    "# Delineate the start of the forecast with a solid black line\n",
    "plt.axvline(x=df['target_end_date'].iloc[-1], color='black', linestyle='--')\n",
    "\n",
    "plt.title('COVID-19 Incident Cases Forecast (log scale)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('log(n. cases)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a672e5",
   "metadata": {},
   "source": [
    "#### b) Now repeat the excercise with a random-walk with drift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an ARIMA model (0,1,0) with a constant term to model a random walk with drift\n",
    "model = ARIMA(df['log_truth_value'], order=(0, 1, 0), trend='t')\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast the next 365 days\n",
    "forecast = ...\n",
    "\n",
    "# Extract the forecast mean and confidence intervals\n",
    "forecast_mean = ...\n",
    "conf_int = ...  # 95% confidence interval\n",
    "\n",
    "# Calculate the 70% and 90% confidence intervals\n",
    "conf_int_70 = ...  # 70% confidence interval\n",
    "conf_int_90 = ...  # 90% confidence interval\n",
    "\n",
    "# Convert forecast index to match the dates if necessary\n",
    "forecast_dates = ...\n",
    "\n",
    "# Plot the historical data\n",
    "plt.figure(figsize=(10, 6))\n",
    "...\n",
    "\n",
    "# Plot the forecast\n",
    "...\n",
    "\n",
    "# Plot the confidence intervals\n",
    "...\n",
    "\n",
    "# Delineate the start of the forecast with a solid black line\n",
    "...\n",
    "\n",
    "plt.title('COVID-19 Incident Cases Forecast with Drift (log scale)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('log(n. cases)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9826b4",
   "metadata": {},
   "source": [
    "#### c) From the Random Walk with dirft that you just fit, using the fact that for a univariate random variable we can make inference simply by looking at its marginal distribution, assume a normal distribution and calculate the probability that there will be more than 5 million cases of covid one-year-on from the last observed case in the dataset. \n",
    "\n",
    "Hint: This requires you to first estimate the standard deviation of the posterior predictive distribution -- remember the 95% interval is the same as 1.96 standard deviations from the mean on each side for a normal distribution. Then simulate 10000 values from the normal with the ppoint estimate forecast value as the mean, and the estimated standard deviation. Then convert the log-values back to the original scale, and calculate the % of the simulated original-scale values that lie above 5 million. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last row (which corresponds to the forecast interval you're interested in)\n",
    "...\n",
    "\n",
    "# Calculate the range by subtracting the lower limit from the upper limit\n",
    "...\n",
    "\n",
    "# Estimate the standard deviation by dividing the range by 4\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define n_simulations\n",
    "...\n",
    "\n",
    "# Simulating 10000 future log-values using the normal distribution\n",
    "simulated_log_values = ...\n",
    "\n",
    "# Convert the log-values back to the original scale\n",
    "simulated_values = ...\n",
    "\n",
    "# Calculate the percentage of simulated values that are above 5 million\n",
    "probability_above_5_million = ...\n",
    "\n",
    "probability_above_5_million"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4980c",
   "metadata": {},
   "source": [
    "#### d) Is the drift parameter statistically significant significant ? Answer this question by plotting the posterior density of the drift parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4a8b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal, bernoulli, beta, norm\n",
    "\n",
    "# Extract the coefficients (betas) and their covariance matrix from the logistic regression fit\n",
    "beta_mean = ...\n",
    "beta_cov = ...\n",
    "\n",
    "# Simulate beta coefficients\n",
    "simulated_betas = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability that the distribution is above 0\n",
    "prob_above_0 = ...\n",
    "\n",
    "# Plot the distribution of beta\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb242d38",
   "metadata": {},
   "source": [
    "#### Part III: Fitting ARIMA models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58768dd9",
   "metadata": {},
   "source": [
    "#### a) Using the `auto_arima` function, detect the optimal number of AR and MA components necessary to model  the series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1470502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Detecting the optimal ARIMA parameters\n",
    "model = auto_arima(..., start_p=0, start_q=0,\n",
    "                   test='adf',       # Use adf test to find optimal 'd'\n",
    "                   max_p=5, max_q=5, # Maximum p and q\n",
    "                   m=1,              # Frequency of the series\n",
    "                   d=None,           # Let the model determine 'd'\n",
    "                   seasonal=False,   # No Seasonality\n",
    "                   start_P=0, \n",
    "                   D=0, \n",
    "                   trace=True,\n",
    "                   error_action='ignore',  \n",
    "                   suppress_warnings=True, \n",
    "                   stepwise=True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624b50e",
   "metadata": {},
   "source": [
    "#### b) Generate forecasts for the next year using this model. Display the 70, 90 and 95% intervals as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b645b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the selected model order\n",
    "order = model.order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23fcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-seasonal ARIMA model\n",
    "model = ARIMA(..., order=order)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast the next 365 days\n",
    "forecast = model_fit.get_forecast(steps=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the forecast mean and confidence intervals\n",
    "forecast_mean = ...\n",
    "conf_int = ...  # 95% confidence interval\n",
    "\n",
    "# Calculate the 70% and 90% confidence intervals\n",
    "conf_int_70 = ...  # 70% confidence interval\n",
    "conf_int_90 = ...  # 90% confidence interval\n",
    "\n",
    "# Convert forecast index to match the dates if necessary\n",
    "forecast_dates = ...\n",
    "\n",
    "# Plot the historical data\n",
    "...\n",
    "\n",
    "# Plot the forecast\n",
    "...\n",
    "\n",
    "# Plot the confidence intervals\n",
    "...\n",
    "\n",
    "# Delineate the start of the forecast with a solid black line\n",
    "...\n",
    "\n",
    "plt.title('COVID-19 Incident Cases Forecast with Drift (log scale)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('log(n. cases)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaef1fa",
   "metadata": {},
   "source": [
    "#### b) Now, using a temporally-aware cross-validation, get the point-estimate of the generalisation error of the selected model. Ensure you get the generalisation error on the origincal scale so that it is interpretable by policy makers. \n",
    "\n",
    "Note: Why must we make edits to traditional cross-validation ? Because we are not interested in `interpolation` but `extrapolation` -- so we want each fold to have a `future time` it has to forecast, rather than just random time points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e303200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a manual function:\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "data = df['log_truth_value']  # Your time series data\n",
    "\n",
    "n_splits = ... # determine the number of cv splits \n",
    "total_size = ... # size of your data (T)\n",
    "split_size = ...  # Determining roughly equal-sized splits\n",
    "\n",
    "scores = [] # bucket to put the scores in \n",
    "\n",
    "for split in range(1, n_splits + 1):\n",
    "    \n",
    "    # Calculate the index to split the data\n",
    "    split_index = split_size * split\n",
    "    \n",
    "    # Ensure we do not go beyond the dataset's size\n",
    "    # (this can happen due to the `roughly' equallty sized)\n",
    "    if split_index > total_size:\n",
    "        break\n",
    "    \n",
    "    # Splitting the data into training and test sets\n",
    "    train, test = data[...], ...(data[...]) # ensure here we are looking at testing on the original scale\n",
    "    \n",
    "    # Fit the selected model on the training data\n",
    "    model = ...\n",
    "    model_fit = ...\n",
    "    \n",
    "    # Forecast the next time steps equivalent to the size of the test set\n",
    "    forecast = ...(model_fit.forecast(steps=len(test)))\n",
    "    \n",
    "    # Calculate and store the RMSE\n",
    "    mse = ...\n",
    "    rmse = ... # Calculating RMSE\n",
    "    scores.append(rmse)\n",
    "\n",
    "# Calculate average RMSE\n",
    "average_score = np.mean(scores)\n",
    "print(f'Average RMSE: {average_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acfd6e2",
   "metadata": {},
   "source": [
    "#### c) This cross-validation method above provides a general estimate of the forecast error. But this is not so useful because we know many-steps-ahead forecasts will be worse than few-steps-ahead. Edit the above to use each cross-validation split to estimate up to 30 steps ahead worth of generalisation error. Store this in a new object. Plot this against the steps-ahead to see how the error can be expected to evolve over time. \n",
    "\n",
    "Note: there might not be enough observations to check 30 steps ahead in each fold -- check as many as you can ! \n",
    "\n",
    "The below I give you `for free`. Play around with changing n_splits, and try to understand the underlying plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e633ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = df['log_truth_value']\n",
    "\n",
    "n_splits = 5  # Define the number of splits for cross-validation\n",
    "max_forecast_horizon = 30  # Maximum forecast horizon -- \n",
    "# note: In the dataset we have weekly readings. Here we are testing the accuracy of the forecasts\n",
    "# for each of these readings, so the horizon is implicitly at the week-level. \n",
    "errors = []  # Using a list to collect arrays of varying lengths\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    train, test = data.iloc[train_index], np.exp(data.iloc[test_index])\n",
    "    available_forecast_horizon = min(len(test), max_forecast_horizon)\n",
    "    \n",
    "    model = ARIMA(train, order=order)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    forecast = np.exp(model_fit.forecast(steps=available_forecast_horizon))\n",
    "    true_values = test.iloc[:available_forecast_horizon].reset_index(drop=True)\n",
    "    \n",
    "    fold_errors = np.zeros(available_forecast_horizon)\n",
    "    for step in range(available_forecast_horizon):\n",
    "        # Ensuring correct indexing for both forecast and true_values\n",
    "        # Convert forecast to a numpy array if it's not already one to standardize indexing\n",
    "        forecast_values = np.array(forecast) if not isinstance(forecast, np.ndarray) else forecast\n",
    "        mse = mean_squared_error([true_values.iloc[step]], [forecast_values[step]])\n",
    "        rmse = np.sqrt(mse)\n",
    "        fold_errors[step] = rmse\n",
    "    \n",
    "    errors.append(fold_errors)\n",
    "\n",
    "# To handle varying lengths, we need to average errors for each step across all folds differently\n",
    "max_length = max(len(e) for e in errors)  # Find the maximum forecast horizon across folds\n",
    "average_errors = np.zeros(max_length)\n",
    "for i in range(max_length):\n",
    "    step_errors = [e[i] for e in errors if i < len(e)]  # Collect ith error from each fold if it exists\n",
    "    average_errors[i] = np.mean(step_errors)  # Average those errors\n",
    "\n",
    "# Plot the generalization error as a function of forecast horizon\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_length + 1), average_errors, marker='o')\n",
    "plt.title('Generalization Error up to 30 Steps Ahead')\n",
    "# Manually format y-axis labels to avoid scientific notation\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.2f}'.format(x)))\n",
    "plt.xlabel('Forecast Horizon (Steps Ahead)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cff281",
   "metadata": {},
   "source": [
    "There are other ways to conduct this sort of generalisation error estimation -- see here for example: https://openforecast.org/adam/rollingOrigin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0f710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8951b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd875c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
