{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4b07a8",
   "metadata": {},
   "source": [
    "# Foundations of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339f782",
   "metadata": {},
   "source": [
    "## Exercise 1: Understanding the Likelihood of Observations Under a Given Gaussian Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f06a5",
   "metadata": {},
   "source": [
    "### Question 1a: Gaussian Likelihood Function\n",
    "Write a function to compute the likelihood of observing a set of values under a normal distribution with mean mu and variance sigma^2. The function should accept an observation Y, a mean value mu, and a standard deviation sigma, and return the likelihood of observing Y under these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f84aac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3989422804014327"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# The likelihood of y given mu and sigma\n",
    "def normal_pdf(y, mu, sigma):\n",
    "    return (1 / math.sqrt(2 * math.pi * (sigma**2))) * math.exp((-1 / 2 * (sigma**2)) * ((y - mu)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8abc44",
   "metadata": {},
   "source": [
    "### Question 1b: Likelihood Under Standard Normal Model\n",
    "Simulate a sequence of values from -5 to 5 (with a step of 0.1) and calculate the likelihood of each value under a Standard Normal model (mu=0, sigma=1). Plot these likelihood values and discuss the shape of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c53645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af180f",
   "metadata": {},
   "source": [
    "## Exercise 2: Playing with Linear Regression\n",
    "\n",
    "We will work with the `Time-for-change` regression model to predict the results of the US elections. The goal of the model is to estimate the % of the two-party (Republican + Democrat) vote from 3 variables: the `incumbency` (i.e. whether the incumbent party candidate is seeking re-election or if it's a new candidate) of the president; the growth rate of the economy; and the approval rating of the incumbent party president. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c423554",
   "metadata": {},
   "source": [
    "### Question 2a: Linear Regression Model\n",
    "Estimate a linear regression model to predict vote share using at least one categorical variable (if applicable) and one numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba41ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change working directory to where you store the Camp files \n",
    "os.chdir('/Users/Sari/Documents/Code/Semester 4 workbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ee7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load Historical Election Results dataset\n",
    "df = pd.read_csv('Camp_Election.Oracle/Historical Election Results/MIT_Lab_Historical_Election_Results/1976-2020-president.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65204f6e",
   "metadata": {},
   "source": [
    "Here I want to include in this dataset the necessary tools to perform the `time for change` model. I need to augment the dataset with a) the `incumbency` of the party / president ; b) the real GDP growth in the year of the election; c) the `approval rating` of the incumbent president on the eve of the election. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do a little data Wrangling to improve the dataset for analysis\n",
    "\n",
    "# Focus on Democrats and Republicans only \n",
    "df = df[df['party_simplified'].str.contains('DEMOCRAT') | df['party_simplified'].str.contains('REPUBLICAN')]\n",
    "\n",
    "# Calculate the two-party total votes for each state and year\n",
    "df['two_party_total'] = df.groupby(['year', 'state'])['candidatevotes'].transform('sum')\n",
    "\n",
    "# Calculate the percentage of the two-party vote afforded to each party\n",
    "df['pct_2p_vote'] = 100*df['candidatevotes'] / df['two_party_total']\n",
    "\n",
    "# define a function that returns the incumbent and whether the incumbent party is seeking their 2nd term in office\n",
    "def identify_incumbent_complete(year):\n",
    "    if year in [1976]:\n",
    "        return ('REPUBLICAN', 0)  # Ford (Continuing Nixon's term)\n",
    "    elif year in [1980]:\n",
    "        return ('DEMOCRAT', 1)  # Carter\n",
    "    elif year in [1984, 1988]:\n",
    "        return ('REPUBLICAN', 1 if year == 1984 else 0)  # Reagan\n",
    "    elif year in [1992]:\n",
    "        return ('REPUBLICAN', 0)  # H.W. Bush (continuing Republican (Reagan) term)\n",
    "    elif year in [1996, 2000]:\n",
    "        return ('DEMOCRAT', 1 if year == 1996 else 0)  # Clinton\n",
    "    elif year in [2004, 2008]:\n",
    "        return ('REPUBLICAN', 1 if year == 2004 else 0)  # W. Bush\n",
    "    elif year in [2012, 2016]:\n",
    "        return ('DEMOCRAT', 1 if year == 2012 else 0)  # Obama\n",
    "    elif year in [2020]:\n",
    "        return ('REPUBLICAN', 1)  # Trump\n",
    "    else:\n",
    "        return ('None', 0)  # Default for years without a U.S. presidential election\n",
    "\n",
    "# Apply the function to each row\n",
    "df['incumbent_party'] = 'None'\n",
    "df['term2'] = 0\n",
    "for index, row in df.iterrows():\n",
    "    incumbent_party, term2 = identify_incumbent_complete(row['year'])\n",
    "    df.at[index, 'incumbent_party'] = incumbent_party\n",
    "    df.at[index, 'term2'] = term2\n",
    "\n",
    "# Now we can focus on the incumbent party, and focus our analysis to ask `what is the chance that the share of the vote the incmumbent will get ? \n",
    "df['incumbent_dummy'] = np.where(df['incumbent_party']==df['party_simplified'],1,0)\n",
    "df = df[df['incumbent_dummy'] == 1]\n",
    "\n",
    "# and we only need a few variables \n",
    "df = df[['year','state','pct_2p_vote','term2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ec76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment with national GDP data \n",
    "aux_gdp = pd.read_csv('Camp_Election.Oracle/Historical_National.US.GDP.csv')\n",
    "aux_gdp = aux_gdp[aux_gdp['Items']=='Gross domestic product']\n",
    "\n",
    "# melt \n",
    "aux_gdp = pd.melt(aux_gdp, id_vars='Items',var_name='year',value_name='real_gdp_pct_growth')\n",
    "aux_gdp = aux_gdp[['year','real_gdp_pct_growth']]\n",
    "aux_gdp['real_gdp_pct_growth'] = pd.to_numeric(aux_gdp['real_gdp_pct_growth'])\n",
    "\n",
    "# merge with df \n",
    "df['year'] = df['year'].astype(int)  # Convert to int\n",
    "aux_gdp['year'] = aux_gdp['year'].astype(int)  # Ensure this matches df\n",
    "df = pd.merge(df, aux_gdp, on='year', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fedec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment with incumbent's approval rating\n",
    "aux_app = pd.read_csv('Camp_Election.Oracle/Historical_President.Approval.Polls_Gallup.csv')\n",
    "aux_app['net_approval'] = aux_app['Approving'] - aux_app['Disapproving']\n",
    "\n",
    "# select only the approval polls right closest to the election\n",
    "import pandas as pd\n",
    "\n",
    "def last_approval_before_election(data):\n",
    "    \"\"\"\n",
    "    Selects the last approval poll before each presidential election in the dataset.\n",
    "    \n",
    "    :param data: A pandas DataFrame with columns 'Start Date', 'End Date', 'Approving', 'Disapproving', 'Unsure/NoData', and 'net_approval'.\n",
    "    :return: A pandas DataFrame with the selected polls.\n",
    "    \"\"\"\n",
    "    # Convert date columns to datetime\n",
    "    data['Start Date'] = pd.to_datetime(data['Start Date'])\n",
    "    data['End Date'] = pd.to_datetime(data['End Date'])\n",
    "    \n",
    "    # Sort the data by 'End Date' to ensure chronological order\n",
    "    data.sort_values(by='End Date', inplace=True)\n",
    "    \n",
    "    # Identify the range of years in the dataset\n",
    "    start_year = data['End Date'].dt.year.min()\n",
    "    end_year = data['End Date'].dt.year.max()\n",
    "    \n",
    "    # Presidential elections are held every 4 years, starting from 1944\n",
    "    election_years = [year for year in range(start_year, end_year+1) if year % 4 == 0 and year >= 1944]\n",
    "    \n",
    "    selected_polls = []\n",
    "    for year in election_years:\n",
    "        # Election date: first Tuesday after the first Monday in November\n",
    "        # Simplification: use November 1st as a base, then adjust\n",
    "        base_date = pd.Timestamp(year=year, month=11, day=1)\n",
    "        day_of_week = base_date.dayofweek\n",
    "        election_date = base_date + pd.Timedelta(days=(1-day_of_week) % 7 + 1)\n",
    "        \n",
    "        # Select the last poll before the election\n",
    "        last_poll = data[data['End Date'] < election_date].iloc[-1]\n",
    "        selected_polls.append(last_poll)\n",
    "    \n",
    "    return pd.DataFrame(selected_polls)\n",
    "\n",
    "# apply the function\n",
    "aux_app = last_approval_before_election(aux_app)\n",
    "\n",
    "# extract relevant year\n",
    "aux_app['year'] = aux_app['End Date'].astype(str).str[:4].astype(int)\n",
    "\n",
    "# drop unnecessary variables \n",
    "aux_app = aux_app[['year','net_approval']]\n",
    "\n",
    "# merge with election results dataset \n",
    "df = pd.merge(df, aux_app, on='year', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4061f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's see the results of our effors\n",
    "print(df)\n",
    "\n",
    "# Write this to a csv so we can use it in later workshops\n",
    "df.to_csv('Workshop_I__Foundations_of_Linear_Regression/generated_quantities/clean_TFC_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab3565",
   "metadata": {},
   "source": [
    "### Now we estimate the `time for change` regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbea9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define your design matrix and your target variable \n",
    "...\n",
    "\n",
    "# Add a constant to the features to account for the intercept in the model\n",
    "...\n",
    "\n",
    "# Fit the model\n",
    "...\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f0a97",
   "metadata": {},
   "source": [
    "### b) Inference by `Empirical Posterior` Simulations\n",
    "\n",
    "Simulate 1000 values from the empirical posterior distribution of each of your coefficients. Plot the histogram for each of these coefficient simulations, and calculate: i. the proportion of simulation that are above 0; ii. the Monte Carlo Median and 95% prediction interval for each coefficient; iii. Comment on the significance and magnitude of the effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92057eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate values from empirical posterior distribution of model coefficients\n",
    "\n",
    "# Hint: use the np.random.multivariate_normal function to simulate form a multivariate normal, \n",
    "# covariance matrix `cov_params()' from the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8691d50",
   "metadata": {},
   "source": [
    "### c) Generate Predicted Values and Estimate Statistics\n",
    "\n",
    "For each record in your dataset, generate predicted values. Use the package which you are using for fitting the regression model to output a) point estimates for each fitted value; b) the expected standard error of those estimates. Use these to generate 1000 samples from the empirical posterior distribution of the fitted values, and estimate the Monte Carlo Median and the 95% prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8041835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Step 1: Prepare the new dataset - remember to add a constant if your model includes an intercept\n",
    "X_new = X\n",
    "...\n",
    "\n",
    "# Step 2: Predict new values (use the model.predict function)\n",
    "...\n",
    "\n",
    "# Step 3: Calculate standard errors of the predictions\n",
    "cov_matrix = ...\n",
    "design_matrix = X_new_with_const\n",
    "variances = np.diag(np.dot(np.dot(design_matrix, cov_matrix), design_matrix.T))\n",
    "std_errors_predictions = ...\n",
    "\n",
    "# Generate posterior samples\n",
    "new_predictions_array = np.array(new_predictions)[:, None]  # Convert to numpy array and add new axis\n",
    "std_errors_predictions_array = np.array(std_errors_predictions)[:, None]  # Convert to numpy array and add new axis\n",
    "\n",
    "predicted_samples = np.random.normal(loc=...,\n",
    "                                     scale=...,\n",
    "                                     size=(..., 1000))\n",
    "\n",
    "monte_carlo_medians = ...\n",
    "prediction_intervals = np.percentile(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4d5e9",
   "metadata": {},
   "source": [
    "### d) Plot Observed vs. Predicted Values with Prediction Intervals\n",
    "\n",
    "Plot the observed values (on the y-axis) against the Monte Carlo Median of the fitted values (x-axis) for each observation. On the same plot, find a way to add the prediction intervals around your fitted values. Make sure the y and x axis span the same support, and add a y = x line to the plot. Comment on what the plot shows - does your model recover good predictions ? How can you tell ? Play around with your model and look how including / removing complexity changes the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34b06d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "# Plotting\n",
    "scatter = plt.scatter(..., y,...)\n",
    "errorbar = plt.errorbar(..., y, xerr=np.abs(prediction_intervals - monte_carlo_medians), ...)\n",
    "y_equals_x = plt.plot(..., 'k--', zorder=3, label='y = x')\n",
    "\n",
    "...\n",
    "\n",
    "plt.title('Observed vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Observed Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abc48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42c8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
