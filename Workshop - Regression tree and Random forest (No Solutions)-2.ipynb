{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Workshop VIII** <br/> *Regression Tree, Random Forest, Bagging and Boosting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide a practical overview of regression trees, random forests, as well as bagging and boosting. After this workshop, the student should be able to know:\n",
    "* how to apply regression trees, random forests, or a bagging / boosting approach\n",
    "* when to apply regression trees, random forests, or a bagging / boosting approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Load your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "df = pd.DataFrame(data= np.c_[diabetes['data'], diabetes['target']], columns=diabetes['feature_names'] + ['target'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. [OPTIONAL] Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your dataset, you can perform different techniques of data preprocessing such as dropping NaN values, create dummy variables or standardize your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess and clean your data (if necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Split the dataset into features (independent variables) and target variable (dependent variable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ... # TODO: Take features\n",
    "y = ... # TODO: Take variable\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = ... # TODO: Define the percentage of the test size after train-test split\n",
    "random_state = ... # TODO: Define random seed for reproducibility\n",
    "\n",
    "X_train, X_test, y_train, y_test = ... # TODO: Use train_test_split method\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use sklearn.tree.DecisionTreeRegressor.\n",
    "\n",
    "If you want to find more information about the hyperparameters that this method uses, please visit: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define an Regression Tree object using DecisionTreeRegression \n",
    "regression_tree = ... # you can experiment with any hyperparameters\n",
    "\n",
    "# TODO: Fit the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions\n",
    "y_pred_cart = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model using MSE\n",
    "mse = ...\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of visualization techniques for qualitative interpretations and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the regression tree using plot_tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(regression_tree, feature_names=list(X.columns), filled=True, rounded=True, proportion=True, precision=2)\n",
    "plt.title(\"Regression Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_cart, color='blue', label='Predictions vs Actual')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Ideal Line')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use the class sklearn.ensemble.RandomForestRegressor.\n",
    "\n",
    "If you want to find more information about the hyperparameters that this method uses, please visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define an Random Forest object using RandomForestRegressor \n",
    "rf_model = ... # you can experiment with any hyperparameters\n",
    "\n",
    "# TODO: Fit the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions\n",
    "y_pred_rf = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model using MSE\n",
    "mse = ...\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understand the parameters of random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importantces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the random forest model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Get the names of features\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# Sort features based on importance\n",
    "sorted_idx = feature_importances.argsort()\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align=\"center\")\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this figure mean? Which feature is the most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_rf, color='green', label='Predictions vs Actual (Random Forest)')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Ideal Line')\n",
    "plt.title('Random Forest Predictions vs Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use fewer or more estimators in random forest? Plot the curve to see how the MSE change for different value of n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nList = [3,5,10,50,100,200,300]\n",
    "mseList = []\n",
    "for nEst in nList:\n",
    "    pass\n",
    "    # TODO: change the number of estimator\n",
    "\n",
    "plt.plot(nList,mseList,\"-o\")\n",
    "plt.title('MSE vs. Number of estimator in random forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following part, we are going to cover two fundamental ensemble learning techniques: **Bagging** and **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bagging - Training and Evaluating a Model of Your Choice\n",
    "The function sklearn.ensemble.BaggingRegressor can be utilized to perform the bagging on your own algorithms.\n",
    "\n",
    "Try to implement it in a way that the base estimator is the linear regression model.\n",
    "\n",
    "If you want to find more information about the hyperparameters that this method uses, please visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# TODO: Defining and fitting the model\n",
    "bagging_model = ...\n",
    "... # for fitting the model\n",
    "\n",
    "# TODO: Make predictions\n",
    "y_pred_bagging = ...\n",
    "\n",
    "# TODO: Evaluate the model using MSE\n",
    "mse_bagging = ...\n",
    "print(f'Mean Squared Error (Bagging): {mse_bagging}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_bagging, color='green', label='Predictions vs Actual (Bagging)')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Ideal Line')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# TODO: Defining and fitting the model\n",
    "adaboost_model = ...\n",
    "... # for fitting the model\n",
    "\n",
    "# TODO: Make predictions\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "\n",
    "# TODO: Evaluate the model\n",
    "mse_adaboost = ...\n",
    "print(f'Mean Squared Error (AdaBoost): {mse_adaboost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_adaboost, color='green', label='Predictions vs Actual (AdaBoost)')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Ideal Line')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You may need to install xgboost first:\n",
    "\n",
    "$ pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Defining and fitting the model\n",
    "xgb_model = ...\n",
    "... # for fitting the model\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_xgb = ...\n",
    "print(f'Mean Squared Error (XGBoost): {mse_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_xgb, color='green', label='Predictions vs Actual (XGBoost)')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Ideal Line')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. [OPTIONAL] Try XGBoost or random forest in classification task.\n",
    "Do you think the above algorithms will work for classification tasks? Next, we try to use them for classification tasks.\n",
    "\n",
    "For simplicity we discretize our y value into two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Reshape y_train to use with KBinsDiscretizer (it expects 2D array)\n",
    "y_test_reshaped = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Initialize the discretizer\n",
    "discretizer = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='kmeans')\n",
    "\n",
    "# Fit the discretizer on the training data\n",
    "discretizer.fit(y_test_reshaped)\n",
    "\n",
    "# Now, transform y_train and y_test using the fitted discretizer\n",
    "y_test_discrete = discretizer.transform(y_test_reshaped).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to split a continous variable by the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_data_median(y_test):\n",
    "    # Calculate the median\n",
    "    median = y_test.median()\n",
    "\n",
    "    y_test_discrete = (y_test > median).astype(int)\n",
    "    return y_test_discrete\n",
    "\n",
    "# Usage:\n",
    "y_test_distrete = discretize_data_median(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use XGBoost to predict their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Defining and fitting the model\n",
    "xgbcls_model = ...\n",
    "\n",
    "# Fit the model\n",
    "...\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgbcls = xgbcls_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgbcls = ...\n",
    "print(f'Accuracy (XGBoost): {accuracy_xgbcls}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use RandomForest to predict their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define a Random Forest object using RandomForestClassifier\n",
    "rfcls_model = ...\n",
    "\n",
    "# Fit the model\n",
    "...\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rfcls = rfcls_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rfcls = ...\n",
    "print(f'Accuracy: {accuracy_rfcls}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpretation of your results\n",
    "\n",
    "Use this space to analyse the performance of your trained models throughout this workshop. Some examples of ideas include:\n",
    "* discussing about the influence of some hyperparameters (e.g.: max_depth for Decision Tree, n_estimators for Random Forest, etc.)\n",
    "* comparing the results you obtained using plots or tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
